import streamlit as st
import os
import json
from langchain_community.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain, LLMChain
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate

st.title("Assistant Code du Travail ")

llm_choice = st.sidebar.selectbox(
    "Choisissez votre mod√®le LLM",
    ("Local (Ollama/Mistral)", "OpenAI (cl√© requise)")
)

if llm_choice == "OpenAI (cl√© requise)":
    openai_api_key = st.sidebar.text_input("OpenAI API Key", type="password")
else:
    openai_api_key = None




def check_and_reset_history(llm_choice: str):
    if "last_model" not in st.session_state:
        st.session_state.last_model = llm_choice
    elif llm_choice != st.session_state.last_model:
        st.session_state.chat_history = []
        st.session_state.last_model = llm_choice


check_and_reset_history(llm_choice)

# load LLMs
@st.cache_resource
def load_llm(llm_choice, openai_api_key=None):
    if llm_choice == "Local (Ollama/Mistral)":
        llm = Ollama(model="mistral:7b-instruct")
    else:
        llm = ChatOpenAI(
            model_name="gpt-3.5-turbo",
            openai_api_key=openai_api_key
        )
    st.sidebar.markdown(f"üîç **Mod√®le LLM utilis√© :** `{llm_choice}`")
    return llm




# load embeddings
@st.cache_resource
def load_embeddings():
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return embeddings

# load vector db
@st.cache_resource
def load_vector_db():
    vector_db = Chroma(
        persist_directory="french_labour_code_vectordb",
        embedding_function=load_embeddings()
    )
    return vector_db

# --- Conversation Memory ---
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# Initialiser l'historique de conversation dans Streamlit
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Charger les composants

llm = load_llm(llm_choice, openai_api_key)

embeddings = load_embeddings()
vector_db = load_vector_db()

# Pr√©parer la m√©moire conversationnelle
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# Cr√©er la cha√Æne conversationnelle
chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vector_db.as_retriever(),
    memory=memory
)


def get_user_input():


    return st.text_input("Votre question", key="user_question", value=st.session_state.get("user_input", ""))

def clean_text(text):
    return text.replace("\n", " ")


# Cr√©er la cha√Æne conversationnelle
custom_prompt = PromptTemplate(
    template="""
    Vous √™tes un assistant juridique intelligent, bienveillant, et expert du Code du travail fran√ßais.

    Votre comportement d√©pend du type de question pos√©e par l'utilisateur :

    ---

    **1. Si la question est une salutation (ex. "salut", "bonjour", "hi") :**  
    ‚Üí R√©pondez chaleureusement, en fran√ßais, pr√©sentez votre r√¥le, et invitez l‚Äôutilisateur √† poser une question juridique li√©e au Code du travail fran√ßais.

    **2. Si la question n‚Äôest pas juridique ou semble hors sujet (ex. "Quel jour sommes-nous ?", "raconte-moi une blague") :**  
    ‚Üí R√©pondez poliment, mais informez que vous √™tes con√ßu uniquement pour r√©pondre √† des questions relatives au Code du travail fran√ßais.

    **3. Si la question est juridique et concerne le Code du travail fran√ßais :**  
    ‚Üí R√©pondez uniquement en vous appuyant sur le **contexte fourni** ci-dessous.  
    ‚Üí Citez **explicitement** les **articles** du Code du travail mentionn√©s dans le contexte.  
    ‚Üí Si aucune information pertinente n‚Äôest trouv√©e dans le contexte, dites clairement que vous ne pouvez pas r√©pondre avec certitude.

    ---

    **Important :**
    - Toutes vos r√©ponses doivent √™tre r√©dig√©es en **fran√ßais**, m√™me si la question est pos√©e dans une autre langue.
    - N‚Äôinventez jamais d‚Äôinformations. Si vous ne savez pas, dites-le.

    ---

    **Question de l'utilisateur :**  
    {question}

    **Contexte juridique (extraits pertinents du Code du travail) :**  
    {context}

    ---

    **R√©ponse :**
    """
    ,
    input_variables=["question", "context"]
)
custom_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vector_db.as_retriever(),
    memory=memory,
    combine_docs_chain_kwargs={"prompt": custom_prompt}
)


def response():
    user_input = get_user_input()
    if user_input:
        result = custom_chain({"question": user_input, "chat_history": st.session_state.chat_history})
        st.session_state.chat_history.append((user_input, result["answer"]))
        st.write(result["answer"])
        # Pour debug‚ÄØ:
        st.write("**Contexte utilis√©‚ÄØ:**", result.get("context", "Aucun contexte trouv√©"))


# Appel de la fonction centrale pour la gestion de la r√©ponse
response()

# Affichage de l'historique dans la sidebar

def creat_chat_history_sidebar():
    with st.sidebar:
        if st.session_state.chat_history:
            st.markdown("### Historique de la conversation :")
            for i, (q, a) in enumerate(st.session_state.chat_history):
                st.markdown(f"**Vous :** {q}")
                st.markdown(f"**Assistant :** {a}")

creat_chat_history_sidebar()
